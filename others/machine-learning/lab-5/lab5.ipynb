{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 5 Lab: Regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Least squares regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1A)\n",
    "With 0th-order basis, we can only have a flat horizontal line, because the $(\\theta, \\theta_0)$ will going to be $(1, \\theta_0)$. We will find the average of the $y$-values because the target\n",
    "$$ \\sum_{i = 1}^n (x^{(i)} - (1 + \\theta_0))^2 $$\n",
    "has the minimum value when \n",
    "$$ 1 + \\theta_0 = \\frac 1 n \\sum_{i = 1}^n x^{(i)} $$\n",
    "by simple calculus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1B)\n",
    "By using 9th-order basis on 10 data points, it is going to produce Lagrangian interpolation through the points because the minimum polynomial order that can go through any $n$ points is an $(n - 1)$-th order polynomial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1C)\n",
    "We will have the risk of getting a weirdly formed prediction and overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1D)\n",
    "It is getting larger and larger as the polynomial order increases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1E)\n",
    "3\\."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1F)\n",
    "4, and unfortunately no."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1G)\n",
    "Yes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1H)\n",
    "No, and no, because the matrix has only rank of 10 and the inverse is not going to do what we wanted earlier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Regularizing the parameter vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2A)\n",
    "With near infinity $\\lambda$, the regression result is like 0th-order basis because it will be penalized so heavy that $\\|\\theta\\|$ has to stay extremely small. On the other hand, if $\\lambda$ is near zero, then the result is still like before. A remarkable observation is that even if $\\lambda$ is at the order of around $10^{-4}$ and $10^{-7}$, the result is still heavily affected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2B)\n",
    "0 if you really want that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2C)\n",
    "0.01."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2D)\n",
    "0.1, and no, and definitely no."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3A)\n",
    "0.03 for 1st-order, and 0.35 for 2nd-order."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3B)\n",
    "With $\\mathtt{max\\_iter = 10000}$, we are able to yield a similar result when $\\eta = 0.25$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3C)\n",
    "With $\\mathtt{max\\_iter = 10000}$, the best is $\\eta = 0.34$ but still not able to reach the really complex graph."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
