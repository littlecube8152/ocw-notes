{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Evaluating diffrerent objectives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[31.5, -1.5, 8.2]\n",
      "[4.0, 0.5, 0.5]\n"
     ]
    }
   ],
   "source": [
    "data = np.array([[1, 2, 1, 2, 10, 10.3, 10.5, 10.7],\n",
    "                 [1, 1, 2, 2,  2,  2,  2, 2]])\n",
    "labels = np.array([[-1, -1, 1, 1, 1, 1, 1, 1]])\n",
    "blue_th = np.array([[0, 1]]).T\n",
    "blue_th0 = -1.5\n",
    "red_th = np.array([[1, 0]]).T\n",
    "red_th0 = -2.5\n",
    "\n",
    "def margin(data, label, th: np.ndarray, th0):\n",
    "    return label * (np.dot(data, th) + th0) / np.linalg.norm(th)\n",
    "\n",
    "red_m = np.array([margin(data[:,i], labels[:,i], red_th, red_th0) for i in range(len(data[0]))])\n",
    "blue_m = np.array([margin(data[:,i], labels[:,i], blue_th, blue_th0) for i in range(len(data[0]))])\n",
    "print([red_m.sum(), red_m.min(), red_m.max()])\n",
    "print([blue_m.sum(), blue_m.min(), blue_m.max()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2 - 5: SVM (support vector machine) \n",
    "In some models, we are not simply seperating data by a slim plane. Instead, we should also maximize the margin bewteen two classes.\n",
    "\n",
    "The idea is that if we have some $\\gamma_{\\mathrm{ref}}$ that we would like to make margin as large as $\\gamma_{\\mathrm{ref}}$. Take a look at the generic objective as \n",
    "$$ J(\\theta, \\theta_0) = \\frac 1 n \\sum L(x^{(i)}, y^{(i)}, \\theta, \\theta_0) + \\lambda R(\\theta, \\theta_0) $$\n",
    "\n",
    "The idea is the seperate the two data groups not by $\\theta^\\top x^{(i)} - \\theta_0 = 0$, but by $\\theta^\\top x^{(i)} - \\theta_0 \\geq 1$ and $\\theta^\\top x^{(i)} - \\theta_0 \\leq 1$. Hence, keep in mind that $\\gamma_{\\mathrm{ref}}$ is **not** like regular $\\gamma$. We measure this by seeing the distance bewteen the two hyperplane as \n",
    "$$ \\gamma_{\\mathrm{ref}} = \\frac 1 2 \\frac 2 {\\|\\theta\\|} $$\n",
    "\n",
    "For loss function, we have several choice of the loss function $L$, one of them not in the clip is *hinge loss*:\n",
    "$$ L_{\\mathrm h}\\left(\\frac\\gamma{\\gamma_{\\mathrm{ref}}}\\right) = \\begin{cases}1 - \\frac\\gamma{\\gamma_{\\mathrm{ref}}} & \\text{if } \\gamma < \\gamma_{\\mathrm{ref}} \\\\ 0 & \\text{else} \\end{cases} $$\n",
    "With the loss in hand, we can write our objective as \n",
    "$$ J(\\theta, \\theta_0) = \\frac 1 n \\sum L_{\\mathrm h}\\left(\\frac{y^{(i)}(\\theta^\\top x^{(i)} + \\theta_0)}{\\|\\theta\\|\\gamma_{\\mathrm{ref}}}\\right) + \\lambda \\frac {1}{\\gamma_{\\mathrm{ref}}^2} $$\n",
    "Because not only we want to minimize the classification error, we also want the margin to be as large as possible.\n",
    "\n",
    "However, it is hard to directly apply optimization algorithms, such as gradient descent, with $\\gamma_{\\mathrm{ref}}$ in the formula. Fortunately, what defines the margin, geometrically, turns out to \n",
    "$$ \\gamma_{\\mathrm{ref}} = \\frac 1 {\\|\\theta\\|} $$\n",
    "\n",
    "Hence we can obtain\n",
    "$$ J(\\theta, \\theta_0) = \\frac 1 n \\sum L_{\\mathrm h}(y^{(i)}(\\theta^\\top x^{(i)} + \\theta_0)) + \\lambda \\|\\theta\\|^2 $$\n",
    "\n",
    "Keep in mind that if $\\lambda$ is zero, the $\\theta$ will grow as much as possible as that *minimizes* our margin. Do note that first term is for *fitness*, and second term is for *significance*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7999999999999998, 0.0, 3.0]\n"
     ]
    }
   ],
   "source": [
    "data = np.array([[1.1, 1, 4],[3.1, 1, 2]])\n",
    "labels = np.array([[1, -1, -1]])\n",
    "th = np.array([[1, 1]]).T\n",
    "th0 = -4\n",
    "\n",
    "def margin(data, label, th, th0):\n",
    "    return label * (np.dot(data, th) + th0) / np.linalg.norm(th)\n",
    "\n",
    "def hinge_loss(data, label, th, th0, g_ref):\n",
    "    return np.float64(max(0, 1 - margin(data, label, th, th0) / g_ref))\n",
    "\n",
    "loss = [hinge_loss(data[:,i], labels[:,i], th, th0, 1 / math.sqrt(2)) for i in range(len(data[0]))]\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 6: SVM w/ Gradient Descent Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradient descent\n",
    "def gd(f, df, x0, step_size_fn, max_iter):\n",
    "    xs = [x0]\n",
    "    fs = [f(x0)]\n",
    "    x = x0.copy()\n",
    "    for i in range(1, max_iter + 1):\n",
    "        x = x - step_size_fn(i) * df(x) \n",
    "        xs.append(x.copy())\n",
    "        fs.append(f(x))\n",
    "    return x, fs, xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 6.999999, -2.      ]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# numerical gradient approximation\n",
    "def num_grad(f, delta=0.001):\n",
    "    def df(x):\n",
    "        d = len(x)\n",
    "        def dx(i):\n",
    "            zero = np.array([[0] * d]).T\n",
    "            zero[i][0] = 1\n",
    "            return zero\n",
    "        return np.array([[(f(x + dx(i) * delta) - f(x - dx(i) * delta)) / (2 * delta) for i in range(d)]]).T\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradient descent w/ numerical gradient\n",
    "def minimize(f, x0, step_size_fn, max_iter):\n",
    "    df = num_grad(f)\n",
    "    xs = [x0]\n",
    "    fs = [f(x0)]\n",
    "    x = x0.copy()\n",
    "    for i in range(1, max_iter + 1):\n",
    "        x = x - step_size_fn(i) * df(x) \n",
    "        xs.append(x.copy())\n",
    "        fs.append(f(x))\n",
    "    return x, fs, xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.15668397]])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hinge loss of computed ratio (v)\n",
    "def hinge(v):\n",
    "    return np.where(v < 1, 1 - v, 0)\n",
    "\n",
    "# hinge loss for seperator with a point\n",
    "def hinge_loss(x, y, th, th0):\n",
    "    return np.average(hinge(y * (np.dot(th.T, x) + th0)))\n",
    "\n",
    "def svm_obj(x, y, th, th0, lam):\n",
    "    return hinge_loss(x, y, th, th0) + lam * float(np.dot(th.T, th))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns the gradient of hinge(v) with respect to v.\n",
    "def d_hinge(v):\n",
    "    return np.where(v < 1, -1, 0)\n",
    "\n",
    "# Returns the gradient of hinge_loss(x, y, th, th0) with respect to th\n",
    "def d_hinge_loss_th(x, y, th, th0):\n",
    "    return d_hinge(y * (np.dot(th.T, x) + th0)) * y * x\n",
    "\n",
    "# Returns the gradient of hinge_loss(x, y, th, th0) with respect to th0\n",
    "def d_hinge_loss_th0(x, y, th, th0):\n",
    "    return d_hinge(y * (np.dot(th.T, x) + th0)) * y\n",
    "\n",
    "# Returns the gradient of svm_obj(x, y, th, th0) with respect to th\n",
    "def d_svm_obj_th(x, y, th, th0, lam):\n",
    "    return np.array([np.average(d_hinge_loss_th(x, y, th, th0), axis=1)]).T + 2 * lam * th\n",
    "\n",
    "# Returns the gradient of svm_obj(x, y, th, th0) with respect to th0\n",
    "def d_svm_obj_th0(x, y, th, th0, lam):\n",
    "    return np.array([np.average(d_hinge_loss_th0(x, y, th, th0), axis=1)])\n",
    "\n",
    "# Returns the full gradient as a single vector (which includes both th, th0)\n",
    "def svm_obj_grad(X, y, th, th0, lam):\n",
    "    return np.vstack((d_svm_obj_th(X, y, th, th0, lam), d_svm_obj_th0(X, y, th, th0, lam)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_svm_min(data, labels, lam):\n",
    "    def svm_min_step_size_fn(i):\n",
    "       return 2/(i+1)**0.5\n",
    "    def f(x):\n",
    "        return svm_obj(data, labels, x[:-1], x[-1:], lam)\n",
    "    def df(x):\n",
    "        return svm_obj_grad(data, labels, x[:-1], x[-1:], lam)\n",
    "    return gd(f, df, np.array([[0] * (len(data) + 1)]).T, svm_min_step_size_fn, 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_svm_num_min(data, labels, lam):\n",
    "    def svm_min_step_size_fn(i):\n",
    "       return 2/(i+1)**0.5\n",
    "    def f(x):\n",
    "        return svm_obj(data, labels, x[:-1], x[-1:], lam)\n",
    "    return num_grad(f, np.array([[0] * (len(data) + 1)]).T, svm_min_step_size_fn, 10)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
